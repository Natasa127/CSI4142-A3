{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f047b185-2ea4-459b-bfd8-2e7d72908b77",
   "metadata": {},
   "source": [
    "# Assignment 3: Predictive analysis - Part 2: Classification\n",
    "## Group 105\n",
    "- Natasa Bolic (300241734)\n",
    "- Brent Palmer (300193610)\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e04694-dad3-47ba-9b89-ac3651685853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e20add1-39a5-403a-8a8d-44f8e9d3e89e",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766e7819-f3b8-4a73-a131-b34bc4fecc9c",
   "metadata": {},
   "source": [
    "## Dataset Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06297945-3380-4ac8-8556-7951b556c887",
   "metadata": {},
   "source": [
    "**Url:** https://www.kaggle.com/datasets/blastchar/telco-customer-churn <br>\n",
    "**Name:** Telco Customer Churn <br>\n",
    "**Author:** The dataset was uploaded to `Kaggle` by a user called `BlastChar`, however `IBM` created the original dataset. <br>\n",
    "**Purpose:** The `Telco Customer Churn` dataset is a synthetic dataset created by `IBM`. The dataset includes data on 7043 customers of the\n",
    "fictitious telecommunications company `Telco`. For each customer, data is provided regarding their demographics, their account information, \n",
    "and the services they have signed up for. The purpose of the dataset is to use the provided customer data to predict customer churn. A column is provided, \n",
    "titled `Churn`, that indicates whether or not the customer left within the last month. Since the churn data is provided, we can easily compare\n",
    "our classification results against the true values. <br>\n",
    "**Shape:** There are 7043 rows and 21 columns. (7043, 21)<br>\n",
    "**Features:** \n",
    "- `customerID` (categorical): A unique ID provided to each customer in the form `1234-ABCDE`.\n",
    "- `gender` (categorical): The gender of the customer (includes `Male` and `Female`).\n",
    "- `SeniorCitizen` (categorical): Indicates if the customer is a senior.\n",
    "    - `0`: The customer is not a senior.\n",
    "    - `1`: The customer is a senior.\n",
    "- `Partner` (categorical): Indicates if the customer has a partner.\n",
    "    - `Yes`: The customer has a partner.\n",
    "    - `No`: The customer does not have a partner. \n",
    "- `Dependents` (categorical): Indicates if the customer has dependents.\n",
    "    - `Yes`: The customer has dependents.\n",
    "    - `No`: The customer does not have dependents. \n",
    "- `tenure` (numerical): The number of months the customer has been with the company [0,72].\n",
    "- `PhoneService` (categorical): Indicates if the customer has a phone service.\n",
    "    - `Yes`: The customer has a phone service.\n",
    "    - `No`: The customer does not have a phone service.\n",
    "- `MultipleLines` (categorical): Indicates if the customer has multiple lines.\n",
    "    - `Yes`: The customer has multiple lines.\n",
    "    - `No`: The customer has one line.\n",
    "    - `No phone service`: The customer does not have phone service.\n",
    "- `InternetService` (categorical): The customer's internet connection method.\n",
    "    - `Fiber optic`: The customer's internet connection uses fiber optic technology.\n",
    "    - `DSL`: The customer's internet connection uses digital subscriber line (DSL) technology.\n",
    "    - `No`: The customer does not have internet.\n",
    "- `OnlineSecurity` (categorical): Indicates if the customer has online security.\n",
    "    - `Yes`: The customer has online security.\n",
    "    - `No`: The customer does not have online security.\n",
    "    - `No internet service`: The customer does not have internet service.\n",
    "- `OnlineBackup` (categorical): Indicates if the customer has online backup.\n",
    "    - `Yes`: The customer has online backup.\n",
    "    - `No`: The customer does not have online backup.\n",
    "    - `No internet service`: The customer does not have internet service. \n",
    "- `DeviceProtection` (categorical): Indicates if the customer has device protection.\n",
    "    - `Yes`: The customer has device protection.\n",
    "    - `No`: The customer does not have device protection.\n",
    "    - `No internet service`: The customer does not have internet service. \n",
    "- `TechSupport` (categorical): Indicates if the customer has tech support.\n",
    "    - `Yes`: The customer has tech support.\n",
    "    - `No`: The customer does not have tech support.\n",
    "    - `No internet service`: The customer does not have internet service.\n",
    "- `StreamingTV` (categorical): Indicates if the customer has streaming TV.\n",
    "    - `Yes`: The customer has streaming TV.\n",
    "    - `No`: The customer does not have streaming TV.\n",
    "    - `No internet service`: The customer does not have internet service.\n",
    "- `StreamingMovies` (categorical): Indicates if the customer has streaming movies.\n",
    "    - `Yes`: The customer has streaming movies.\n",
    "    - `No`: The customer does not have streaming movies.\n",
    "    - `No internet service`: The customer does not have internet service.\n",
    "- `Contract` (categorical): The contract term of the customer.\n",
    "    - `Month-to-month`: The customer pays month-to-month.\n",
    "    - `One year`: The customer has a one-year contract.\n",
    "    - `Two year`: The customer has a two-year contract. \n",
    "- `PaperlessBilling` (categorical): Indicates if the customer has paperless billing.\n",
    "    - `Yes`: The customer has paperless billing.\n",
    "    - `No`: The customer does not have paperless billing.\n",
    "- `PaymentMethod` (categorical): The customer's payment method.\n",
    "    - `Electronic check`: The customer pays by electronic check.\n",
    "    - `Mailed check`: The customer pays by mailed check.\n",
    "    - `Bank transfer (automatic)`: The customer pays automatically by bank transfer.\n",
    "    - `Credit card (automatic)`: The customer pays automatically by credit card.\n",
    "- `MonthlyCharges` (numerical): The amount charged to the customer each month [18.25, 118.75]. \n",
    "- `TotalCharges` (numerical): The total amount charged to the customer [0.0, 999.9].\n",
    "- `Churn` (categorical): Indicates if the customer churned, which means if the customer cancelled their service this month.\n",
    "    - `Yes`: The customer churned.\n",
    "    - `No`: The customer did not churn. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c49324-e71d-42fb-8b5c-c8b1c9f465cd",
   "metadata": {},
   "source": [
    "## Loading Data and Basic Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf2c256-766c-4972-9afc-e904fa36a76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the dataset from a public repository\n",
    "url = \"https://raw.githubusercontent.com/Natasa127/CSI4142-A3/refs/heads/main/Telco-Customer-Churn.csv\"\n",
    "telco_df = pd.read_csv(url)\n",
    "telco_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8df0ca-8318-4eef-b0f4-e0ff89910ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "telco_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9eae4d-d16a-422f-a60c-2a183b6b767a",
   "metadata": {},
   "outputs": [],
   "source": [
    "telco_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c80d788-22f1-44b9-8dd5-e2da0da5d5ed",
   "metadata": {},
   "source": [
    "## Classification Empirical Study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4188bd4-f024-4475-bfbd-c2e7c372a7e2",
   "metadata": {},
   "source": [
    "### (a) Cleaning the data\n",
    "\n",
    "In this section, we will use the validity checks from `Assignment 2` to determine if the data needs to be cleaned. If needed, an imputation method may be used to clean the data. Note that we will be presenting each of the ten validity checks, although they may be in a different order than in assignment 2. They will be clearly enumerated. Furthermore, in `Assignment 2`, we saved a copy of the invalid rows in our validity checks. In this assignment, we will first check the count of invalid rows; if there are none, we will omit the code that saves a copy, and displays invalid rows. If we find the error count to be non-zero, we will then clean it instead of just saving and displaying the rows.\n",
    "\n",
    "We have applied every check to every feature that makes sense for that check."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227f8e62-55df-4555-947c-2ebc454e64e4",
   "metadata": {},
   "source": [
    "#### Validity Check 1: Exact Duplicates\n",
    "\n",
    "We will first check for exact duplicates in the dataset. This check verifies that there are no rows that are identical over all columns.\n",
    "\n",
    "**References:** <br>\n",
    "Exact Duplicates: https://uottawa.brightspace.com/d2l/le/content/490358/viewContent/6620388/View (Slide 27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ed5d14-02ad-491a-a605-d97d6febb325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exact duplicates check\n",
    "\n",
    "# Apply the .duplicated method to the DataFrame to create a Series, with exact duplicates set to True\n",
    "duplicates = telco_df.duplicated()\n",
    "\n",
    "# Print the number of rows that are exact duplicates\n",
    "print(f\"Number of duplicate rows: {duplicates.sum()}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac66c1b5-ebde-4ed8-89fa-c5dc601b8797",
   "metadata": {},
   "source": [
    "Since there are no exact duplicates, no cleaning is required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340e131b-f649-411f-88a8-725cebcf96a0",
   "metadata": {},
   "source": [
    "#### Validity Check 2: Near Duplicate Errors\n",
    "\n",
    "In `Assignment 2`, our near duplicate error checker checked if a row differed only by a synonym in a designated attribute. However, when developing our dataset description, we used `.value_counts()` on each categorical attribute, and we noted that there are no synonyms that would cause this error. Thus, we will be using a different type of near duplicate check in this assignment. Instead, we will be checking if any customer's have the exact same information, differing only by their `customerID`.\n",
    "\n",
    "**References:** <br>\n",
    "Omit a Column: https://sparkbyexamples.com/pandas/pandas-select-all-columns-except-one-column-in-dataframe/#:~:text=Select%20All%20Except%20One%20Column%20Using%20drop()%20Method%20in,axis%3D1%20or%20columns%20param."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc47ac9-7d50-4b07-8744-64959cd1c3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Near duplicates check\n",
    "\n",
    "# Apply the .duplcated method to the DataFrame, excluding the customerID column, to create a Series, with exact duplicates of the remaining columns set to True\n",
    "no_customer_id_duplicates = telco_df.loc[:, telco_df.columns != \"customerID\"].duplicated()\n",
    "\n",
    "# Print the number of rows that are near duplicates\n",
    "print(f\"Number of near duplicate rows: {no_customer_id_duplicates.sum()}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8bfd40-f2eb-4517-80aa-a03b9db171fe",
   "metadata": {},
   "source": [
    "Since there are 22 near duplicate rows, let us further investigate the actual rows to determine how to handle them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a13d40-6669-4c6d-98eb-95365e74b5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the invalid rows\n",
    "invalid_near_duplicate_df = telco_df.loc[no_customer_id_duplicates]\n",
    "\n",
    "# Display the first 3 rows that are near duplicates\n",
    "print(\"Examples of three near duplicate rows:\")\n",
    "invalid_near_duplicate_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb8b975-cfa7-42ab-81a4-42bdc2aa38cc",
   "metadata": {},
   "source": [
    "We believe that despite our efforts to alter our near duplicate checker to identify rows that vary only by `customerID`, these rows **should not be removed.** There are a few reasons for this:\n",
    "\n",
    "- There is no row that occurs more than 3 times; in essence, there is no row that occurs a completely unreasonable number of times (there is only one row that occurs three times, and the rest occur twice).\n",
    "- Given that there are 7043 customers, 22 customers having the same information is plausible. There are a couple reasons why.\n",
    "    - This is a relatively small portion of the overall sample.\n",
    "    - Unlike the example in the course slides of near duplicates, we do not have a column like `Name` that would be highly likely to be distinct. All of the columns are reasonably likely to overlap.\n",
    "        - There are not many options for each column. All of the categorical values only provide a few options, and in many cases the same value would occur across multiple columns (ex: `No internet service` would be shared across 6 columns, so it effectively reduces the number of columns that could be a differentiator). Even the numerical columns are not particularly surprising to overlap—since they represent the prices of services, and presumably `Telco` offers a discrete set of services, the minimal duplication is expected.\n",
    "    - We analyzed all 22 rows that are duplicates, and the values are values that are logically more likely to be repeated; for example, all the customers had a tenure of 1 month, have a month-to-month plan, no dependents, no partner, were (mostly) not seniors, did not have multiple lines, and most of them do not purchase the internet service. Even of those that did purchase the internet service, none of them had any of the packages creating an unlikely combination. These are all relatively common values, and the internet service specifically reduces the number of differentiating columns. Thus, these users with a common service can only be differentiated by demographic information, their payment methods, and their churn, and it is likely that in some cases this would overlap.\n",
    "        - As an illustrative example, if there were 15 people with a tenure of exactly 42 months, and had a very specific combination of internet packages, then it would be more likely they were duplicates.\n",
    "        - To analyze the values of near duplicates, change `invalid_exact_duplicate_df.head(3)` to `invalid_exact_duplicate_df.head(22)`. We have left only 3 examples for brevity.\n",
    "\n",
    "Thus, we have decided not to remove the rows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bfb64f-e3ad-4ad0-a13c-57e5b54aa3a4",
   "metadata": {},
   "source": [
    "#### Validity Check 3: Format errors\n",
    "\n",
    "We will use the format check from `Assignment 2` to verify if the `customerID` is in the correct format. The only change necessary is to change the regex to verify that the `customerID` is in the format `1234-ABCDE`. None of the other features have a specific format to follow.\n",
    "\n",
    "**References:** <br>\n",
    "Regex Rules: https://www.geeksforgeeks.org/perl-regex-cheat-sheet/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fd466c-3f2f-4026-8184-1b6be93b3a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format check\n",
    "\n",
    "# Attribute selection\n",
    "test_attribute = 'customerID'\n",
    "\n",
    "# We will fix the regex for the customerID format (1234-ABCDE)\n",
    "format_regex = r\"^\\d{4}-[A-Z]{5}$\"\n",
    "\n",
    "# Evaluates a single value against a given regex format\n",
    "def format_filter(value, format_regex):\n",
    "    return False if not isinstance(value, str) else bool(re.findall(format_regex, value))\n",
    "\n",
    "# Apply the function to the test attribute, setting invalid formats to True\n",
    "invalid_format = telco_df[test_attribute].apply(\n",
    "    lambda attribute: not format_filter(attribute, format_regex)\n",
    ")\n",
    "\n",
    "# Print the number of rows with invalid formatting on the chosen test attribute\n",
    "print(f\"Number of rows where the {test_attribute} value has an invalid format: {invalid_format.sum()}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53cdf29b-4c88-4374-95a7-206b39ee283b",
   "metadata": {},
   "source": [
    "Since there are no rows with an invalid `customerID` format, no cleaning is required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ea794f-c2b3-4b15-b37b-00a6aeb8f281",
   "metadata": {},
   "source": [
    "#### Validity Check 4: Uniqueness errors\n",
    "\n",
    "We will use the uniqueness check from `Assignment 2` to validate that the `customerID` values are unique. Minimal changes are necessary to the code. We only need to provide the `customerID` feature name, and change the `sales` DataFrame to `telco_df`. None of the other features are intended to be unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586a9d12-34f1-4c5c-b748-f76c7e865f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uniqueness check\n",
    "\n",
    "# Attribute selection\n",
    "test_attributes = \"customerID\"\n",
    "\n",
    "# Store a series of the counts of each value in the chosen column\n",
    "attribute_series_counts = telco_df[test_attribute].value_counts()\n",
    "\n",
    "# Evaluates a single value, checking if it is unique in the chosen column\n",
    "def uniqueness_filter(value, counts):\n",
    "    if pd.isna(value):\n",
    "        return False\n",
    "    if counts[value] == 1:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# Apply the function to the test attribute, setting rows with non-unique values in the designated column to True\n",
    "invalid_uniqueness = telco_df[test_attribute].apply(\n",
    "    lambda attribute: not uniqueness_filter(attribute, attribute_series_counts)\n",
    ")\n",
    "\n",
    "# Print the number of rows with a value that is not unique in the chosen column\n",
    "print(f\"Number of rows where the {test_attribute} value is not unique: {invalid_uniqueness.sum()}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92fb006-d33f-476f-8246-b5727cb54380",
   "metadata": {},
   "source": [
    "Since every `customerID` is unique, no cleaning is required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f0ced1-85e5-4a51-a96d-c07702b33ba6",
   "metadata": {},
   "source": [
    "#### Validity Check 5: Presence check\n",
    "\n",
    "We will use the presence check from `Assignment 2` to validate that there is no missing information in any of the columns. Minimal changes are necessary to the code. Instead of taking a parameter input, we will simply iterate over all of the columns to determine if any of them are missing values. We also added a check for strings of whitespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d67267-7e9f-4ae2-a714-3ba891808e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Presence check\n",
    "\n",
    "# Iterate over each column and perform the presence check on each\n",
    "for column in telco_df:\n",
    "    # Apply pd.isna() to the test attribute, or check if it is a string of whitespace, setting rows with missing values in the designated column to True\n",
    "    invalid_presence = telco_df[column].apply(lambda attribute: (pd.isna(attribute) or (isinstance(attribute, str) and attribute.strip() == \"\")))\n",
    "\n",
    "    # Print the number of rows with a missing value in the chosen test attribute\n",
    "    print(f\"Number of rows where the {column} value is missing: {invalid_presence.sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6c26d1-c17c-47f0-81b9-227516df9be9",
   "metadata": {},
   "source": [
    "There are only missing values in the `TotalCharges` feature. We will investigate this further, first by printing out some examples of invalid rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317325cc-5bcc-4521-bafb-ef4895f3d6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the rows with a missing TotalCharges\n",
    "invalid_presence = telco_df[\"TotalCharges\"].apply(lambda attribute: (pd.isna(attribute) or (isinstance(attribute, str) and attribute.strip() == \"\")))\n",
    "\n",
    "# Save the invalid rows\n",
    "invalid_presence_df = telco_df.loc[invalid_presence]\n",
    "\n",
    "# Display the first 3 rows with a missing value\n",
    "print(f\"Examples of three rows where the TotalCharges value is missing:\")\n",
    "invalid_presence_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097126f1-c0a6-4ed7-9f81-364356dd2b9c",
   "metadata": {},
   "source": [
    "Note that all of the rows that are missing a `TotalCharges` value have a `tenure` of 0. This makes sense; since this is the customer's first month, they have not been charged yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46a1d52-26a2-4b19-94e8-0d97f884f141",
   "metadata": {},
   "outputs": [],
   "source": [
    "telco_df[\"tenure\"].value_counts()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c95417-09d6-4a93-9421-e9664e372c56",
   "metadata": {},
   "source": [
    "There are 11 customers with a `tenure` of zero, corresponding to the number of customers with a missing `TotalCharges` value. Thus, this must be the reason. Let us replace the missing values with a value of 0, as from a logical standpoint, these customers have been charged $0 so far. We will make this change in a copy of the dataset called `cleaned_telco_df`. Any subsequent cleaning will be done using this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55695865-cc6f-47c5-a34e-5990c6bf77a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_telco_df = telco_df.copy()\n",
    "cleaned_telco_df.loc[cleaned_telco_df[\"tenure\"] == 0, \"TotalCharges\"] = 0\n",
    "\n",
    "# Extract the rows with a missing TotalCharges\n",
    "invalid_presence = cleaned_telco_df[\"TotalCharges\"].apply(lambda attribute: (pd.isna(attribute) or (isinstance(attribute, str) and attribute.strip() == \"\")))\n",
    "\n",
    "# Verify that there are no longer missing values\n",
    "print(f\"Number of rows where the TotalCharges value is missing: {invalid_presence.sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9fe04d-d660-4756-863c-1649b6705a69",
   "metadata": {},
   "source": [
    "Now, we have solved the presence error problem by imputing the value of 0 in `cleaned_telco_df`, which is done since the missing values correspond to customers who have not been charged yet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c80f502-ee3b-4763-8d75-99a7ef8ce5d9",
   "metadata": {},
   "source": [
    "#### Validity Check 6: Look-up Check\n",
    "\n",
    "We will use the look-up check from `Assignment 2` to validate that select features' values exist in a corresponding pre-defined finite set of values. Minimal changes are necessary to the code. Instead of taking a parameter input, we define a dictionary containing the relevant features as the keys, and their respective look-up tables as values. Then, we iterate over all of these features and check if their values are included in their look-up tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbe441e-649c-45fe-806b-ea78af10539e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look-up check\n",
    "\n",
    "# Dictionary containing every feature's look-up table of valid values\n",
    "look_up_dict = {\n",
    "    \"gender\": [\"Male\", \"Female\"],\n",
    "    \"SeniorCitizen\": [0, 1],\n",
    "    \"Partner\": [\"Yes\", \"No\"],\n",
    "    \"Dependents\": [\"Yes\", \"No\"],\n",
    "    \"PhoneService\": [\"Yes\", \"No\"],\n",
    "    \"MultipleLines\": [\"Yes\", \"No\", \"No phone service\"],\n",
    "    \"InternetService\": [\"Fiber optic\", \"DSL\", \"No\"],\n",
    "    \"OnlineSecurity\": [\"Yes\", \"No\", \"No internet service\"],\n",
    "    \"OnlineBackup\": [\"Yes\", \"No\", \"No internet service\"],\n",
    "    \"DeviceProtection\": [\"Yes\", \"No\", \"No internet service\"],\n",
    "    \"TechSupport\": [\"Yes\", \"No\", \"No internet service\"],\n",
    "    \"StreamingTV\": [\"Yes\", \"No\", \"No internet service\"],\n",
    "    \"StreamingMovies\": [\"Yes\", \"No\", \"No internet service\"],\n",
    "    \"Contract\": [\"Month-to-month\", \"One year\", \"Two year\"],\n",
    "    \"PaperlessBilling\": [\"Yes\", \"No\"],\n",
    "    \"PaymentMethod\": [\"Electronic check\", \"Mailed check\", \"Bank transfer (automatic)\", \"Credit card (automatic)\"],\n",
    "    \"Churn\": [\"Yes\", \"No\"]\n",
    "}\n",
    "\n",
    "# Checks if a single value is in the look-up table\n",
    "def look_up_filter(value, look_up_table):\n",
    "    return value in look_up_table\n",
    "\n",
    "# Apply the function to every feature, setting rows whose value is not in the look-up table to True\n",
    "for feature in look_up_dict.keys():\n",
    "    invalid_look_up = cleaned_telco_df[feature].apply(\n",
    "        lambda attribute: not look_up_filter(attribute, look_up_dict[feature])\n",
    "    )\n",
    "    # Print the number of rows with a value that is not in the look-up table for the designated attribute\n",
    "    print(f\"Number of rows where the {feature} value is not in the look-up table: {invalid_look_up.sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643dbb12-24fd-4b1a-be15-7081c0cd77c0",
   "metadata": {},
   "source": [
    "Since none of the features have a row with a value not in the corresponding look-up table, no cleaning is required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a53d886-8d22-4223-a66a-79feb14483a2",
   "metadata": {},
   "source": [
    "#### Validity Check 7: Consistency Check\n",
    "\n",
    "We will use the consistency check from `Assignment 2` to validate that data in a row follows some designated rule that involves multiple columns. Minimal changes are necessary to the code. We simply specify an array of two different rules we want to apply to our dataset, and iterate over them to perform the validation. There are three consistency checks that make sense for this dataset:\n",
    "1. If a user does not have the phone service (value of `No` for `PhoneService`), then they must have a value of `No phone service` in the `MultipleLines` feature. Conversely, those with phone service (value of `Yes` for `PhoneService`) must not have a value of `No phone service` in the `MultipleLines` feature.\n",
    "2. If a user does not have the internet service (values of `No` for `InternetService`), then they must have a value of `No internet service` in the `OnlineSecurity`, `OnlineBackup`, `DeviceProtection`, `TechSupport`, `StreamingTV`, and `StreamingMovies` features. Conversely, those with the internet service (value of `Fiber optic` or `DSL` for `InternetService`) must not have a value of `No internet service` in the `OnlineSecurity`, `OnlineBackup`, `DeviceProtection`, `TechSupport`, `StreamingTV`, and `StreamingMovies` features.\n",
    "3. If a user has a `tenure` of `0`, then their `TotalCharges` must be `0`. Otherwise, their `TotalCharges` must be greater than `0`.\n",
    "\n",
    "**References:** <br>\n",
    "Eval: https://docs.python.org/3/library/functions.html#eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a738863-3108-493e-b7e7-f434353b0260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consistency check\n",
    "\n",
    "# An array containing the three rules described above\n",
    "rules = [\n",
    "    (\n",
    "        '(row[\"InternetService\"] == \"No\" and row[\"OnlineSecurity\"] == row[\"OnlineBackup\"] == row[\"DeviceProtection\"]' \n",
    "        '== row[\"TechSupport\"] == row[\"StreamingTV\"] == row[\"StreamingMovies\"] == \"No internet service\") or '\n",
    "        '(row[\"InternetService\"] in [\"Fiber optic\", \"DSL\"] and row[\"OnlineSecurity\"] in [\"Yes\", \"No\"] and '\n",
    "        'row[\"OnlineBackup\"] in [\"Yes\", \"No\"] and row[\"DeviceProtection\"] in [\"Yes\", \"No\"] and row[\"TechSupport\"] in [\"Yes\", \"No\"] and '\n",
    "        'row[\"StreamingTV\"] in [\"Yes\", \"No\"] and row[\"StreamingMovies\"] in [\"Yes\", \"No\"])'\n",
    "    ),\n",
    "    '(row[\"PhoneService\"] == \"No\" and row[\"MultipleLines\"] == \"No phone service\") or (row[\"PhoneService\"] == \"Yes\" and row[\"MultipleLines\"] in [\"Yes\", \"No\"])',\n",
    "    '(row[\"tenure\"] == 0 and row[\"TotalCharges\"] == 0) or (row[\"tenure\"] > 0 and row[\"TotalCharges\"] > 0)'\n",
    "]\n",
    "\n",
    "# Evalutes a single row against a given rule\n",
    "def consistency_filter(row, rule):\n",
    "    try:\n",
    "        # Convert the row to a dictionary, since eval() can use a dictionary\n",
    "        row = row.to_dict()\n",
    "\n",
    "        # Convert the numeric attributes to float to allow for expression evaluation\n",
    "        for key, value in row.items():\n",
    "            try:\n",
    "                row[key] = float(value)\n",
    "            except Exception as e:\n",
    "                pass\n",
    "\n",
    "        # Return the result of the expression\n",
    "        return eval(rule)\n",
    "    except Exception as e:\n",
    "        # If the eval fails (for example, 2.0 * \"error\" will throw an error), return False\n",
    "        return False\n",
    "\n",
    "# Validate each rule\n",
    "for rule in rules:\n",
    "    # Apply the function to each row, setting inconsistent rows to True\n",
    "    invalid_consistency = cleaned_telco_df.apply(\n",
    "        lambda row: not consistency_filter(row, rule),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Print the number of inconsistent rows based on the provided rule\n",
    "    print(f\"Number of rows where the rule {rule} is invalid: {invalid_consistency.sum()}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4271dec9-b1e0-433f-9bd9-d4f393f8d56a",
   "metadata": {},
   "source": [
    "Since none of the rows are inconsistent for any of the rules, no cleaning is required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28dad8a-5ff7-44b3-8251-e8855fb91fe2",
   "metadata": {},
   "source": [
    "#### Validity Check 8: Data Type Check\n",
    "\n",
    "We will use the data type check from `Assignment 2` to validate that the data stored is the correct data type. We can apply this to every column. We will use our method 1 from `Assignment 2`, which simply checks if the data in its current format has the desired data type. Minimal changes are necessary to the code. We simply specify a dictionary, where the keys are the features, and the values are the desired data types. We iterate over each feature in the dictionary, applying the data type check to each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b900171a-480c-40f7-a155-3c50be1dc312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data type check\n",
    "\n",
    "# Evalutes a single value's data type against the desired data type\n",
    "def type_filter_method1(value, test_datatype):\n",
    "    if pd.isna(value):\n",
    "        return False\n",
    "    return isinstance(value, test_datatype)\n",
    "\n",
    "# Create a dictionary that maps each attribute to its correct datatype\n",
    "data_type_dict = {\n",
    "    \"customerID\": str,\n",
    "    \"gender\": str,\n",
    "    \"SeniorCitizen\": int,\n",
    "    \"Partner\": str,\n",
    "    \"Dependents\": str,\n",
    "    \"tenure\": int,\n",
    "    \"PhoneService\": str,\n",
    "    \"MultipleLines\": str,\n",
    "    \"InternetService\": str,\n",
    "    \"OnlineSecurity\": str,\n",
    "    \"OnlineBackup\": str,\n",
    "    \"DeviceProtection\": str,\n",
    "    \"TechSupport\": str,\n",
    "    \"StreamingTV\": str,\n",
    "    \"StreamingMovies\": str,\n",
    "    \"Contract\": str,\n",
    "    \"PaperlessBilling\": str,\n",
    "    \"PaymentMethod\": str,\n",
    "    \"MonthlyCharges\": float,\n",
    "    \"TotalCharges\": float,\n",
    "    \"Churn\": str\n",
    "}\n",
    "\n",
    "# Apply the function to every feature, setting rows whose value is not stored as the correct datatype to True\n",
    "for feature in data_type_dict.keys():\n",
    "    invalid_datatype = cleaned_telco_df[feature].apply(\n",
    "        lambda attribute: not type_filter_method1(attribute, data_type_dict[feature])\n",
    "    )\n",
    "    # Print the number of rows with a value that is not stored as the correct datatype for the designated attribute\n",
    "    print(f\"Number of rows where the {feature} value is not stored as the correct datatype ({data_type_dict[feature]}): {invalid_datatype.sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704f2a7c-4106-47b3-a592-81c0fe719d41",
   "metadata": {},
   "source": [
    "The only feature with an issue is `TotalCharges`. It should be stored as a `float`, but it is stored as an `object`. This is expected, as we noted earlier that the 11 customer's that had a `tenure` of `0` were initially stored as empty strings instead of `0`. Since we have already converted the empty strings to `0`, we can simply cast the values to the `float` datatype, since the `objects` hold `float` values.\n",
    "\n",
    "**References:** <br>\n",
    "Cast a Column: https://stackoverflow.com/questions/48094854/pandas-convert-data-type-from-object-to-float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f28978b-12ea-4713-8783-ec2f1a19e0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast the TotalCharges feature to the float data type\n",
    "cleaned_telco_df[\"TotalCharges\"] = cleaned_telco_df.TotalCharges.astype(float)\n",
    "\n",
    "# Rerun the validity check to verify the cleaning worked.\n",
    "\n",
    "invalid_datatype = cleaned_telco_df[\"TotalCharges\"].apply(\n",
    "    lambda attribute: not type_filter_method1(attribute, float)\n",
    ")\n",
    "\n",
    "# Print the number of rows with a value that is not stored as the correct datatype for the designated attribute\n",
    "print(f\"Number of rows where the TotalCharges value is not stored as the correct datatype (float): {invalid_datatype.sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188d1d94-ee40-44ff-b885-07f2a9918d1d",
   "metadata": {},
   "source": [
    "Now, we have solved the data type error problem by casting the `TotalCharges` column to `float` in cleaned_telco_df. The dataset has now been cleaned for the data type check."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d680a9bc-0cbb-4f13-95bc-6c49e1b57557",
   "metadata": {},
   "source": [
    "#### Validity Check 9: Length Check\n",
    "\n",
    "We will use the length check from `Assignment 2` to validate that the length of the categorical values is within a designated range. Minimal changes are necessary to the code. Instead of taking a parameter input, we define a dictionary containing the relevant features as the keys, and their respective minimum and maximum lengths as values (stored as a tuple, (`min`, `max`)). Then, we iterate over all of these features and check if their length values are within the designated range.\n",
    "\n",
    "Our minimum and maximum length values are based on the minimum and maximum lengths of the valid values. For example, for `Partner`, the valid values are `Yes` and `No`, so we have set the minimum valid length to `2` and the maximum valid length to `3` accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665f813d-8c17-4b84-956b-7c27b9240150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length check\n",
    "\n",
    "# Dictionary containing every feature's minimum and maximum valid lengths\n",
    "length_dict = {\n",
    "    \"customerID\": (10, 10), # All customerIDs are exactly 10 characters\n",
    "    \"gender\": (4, 6), # Male, Female\n",
    "    \"Partner\": (2, 3), # No, Yes\n",
    "    \"Dependents\": (2, 3), # No, Yes\n",
    "    \"PhoneService\": (2, 3), # No, Yes\n",
    "    \"MultipleLines\": (2, 16), # No, No phone service\n",
    "    \"InternetService\": (2, 11), # No, Fiber optic\n",
    "    \"OnlineSecurity\": (2, 19), # No, No internet service\n",
    "    \"OnlineBackup\": (2, 19), # No, No internet service\n",
    "    \"DeviceProtection\": (2, 19), # No, No internet service\n",
    "    \"TechSupport\": (2, 19), # No, No internet service\n",
    "    \"StreamingTV\": (2, 19), # No, No internet service\n",
    "    \"StreamingMovies\": (2, 19), # No, No internet service\n",
    "    \"Contract\": (8, 14), # One year, Month-to-month\n",
    "    \"PaperlessBilling\": (2, 3), # No, Yes\n",
    "    \"PaymentMethod\": (12, 25), # Mailed check, Bank transfer (automatic)\n",
    "    \"Churn\": (2, 3) # No, Yes\n",
    "}\n",
    "\n",
    "# Evaluates a single value's length against the given valid range of lengths\n",
    "def length_filter(value, minimum_length, maximum_length):\n",
    "    if pd.isna(value) or not isinstance(value, str):\n",
    "        return False\n",
    "    return minimum_length <= len(value) <= maximum_length\n",
    "\n",
    "# Apply the function to every feature in the dictionary, setting invalid length values to True\n",
    "for feature in length_dict.keys():\n",
    "    # Apply the function to the test attribute, setting out of range length values to True\n",
    "    invalid_length = cleaned_telco_df[feature].apply(\n",
    "        lambda attribute: not length_filter(attribute, length_dict[feature][0], length_dict[feature][1])\n",
    "    )\n",
    "\n",
    "    # Print the number of rows with a length value outside of the given valid length range for the designated attribute\n",
    "    print(f\"Number of rows where the {feature} value's length is outside of the defined range of valid lengths {length_dict[feature]}: {invalid_length.sum()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e46fb95-9ef5-4c70-8f6c-b8f3a7be24ad",
   "metadata": {},
   "source": [
    "Since none of the values fail the length check, no cleaning is required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f847a7-2c00-4e4a-a76b-8559e930855c",
   "metadata": {},
   "source": [
    "#### Validity Check 10: Range Check\n",
    "\n",
    "We will use the range check from `Assignment 2` to validate that the values of the numerical values are within the minimum and maximum acceptable values. Minimal changes are necessary to the code. Instead of taking a parameter input, we define a dictionary containing the relevant features as the keys, and their respective minimum and maximum values as values (stored as a tuple, (`min`, `max`)). Then, we iterate over all of these features and check if their values are within the designated range.\n",
    "\n",
    "The following is a rationale for each of the ranges provided:\n",
    "- `tenure` [0, 900]: `Telco` is a fictitious company, so there is no real founding date. Let us suppose that `Telco` is a legacy telecommunications company, established in 1950. Then, the maximum `tenure` possible would be `900` months. A `tenure` cannot be negative, so the minimum is `0`, representing customers in their first month of service.\n",
    "- `MonthlyCharges` [5, 297]: Since `Telco` is a fictious company, there is not a set of services that can be checked to determine minimum and maximum values. From some industry research, we found a company that offers a home phone service for `5` dollars, so we will set the minimum monthly charge to `5` dollars. We are setting the maximum based on Bell's current maximum home phone price plus Bell's maximum internet service price, which is (`137` (Home Phone with Five Lines) + `160` (Gigabit Fibe 8.0)). Technically customers could buy more than five phone lines, but this estimate should be sufficient for this assignment.\n",
    "- `TotalCharges` [0, 267300]: The minimum `TotalCharges` is the minimum `tenure` multiplied by the minimum `MonthlyCharges`, which is `0` due to a possible `tenure` of `0`. The maximum `TotalCharges` is based on the maximum `tenure` multiplied by the maximum `MonthlyCharges`, which is `267300`.\n",
    "\n",
    "**References:** <br>\n",
    "5-dollar Home Phone: https://www.fongo.com/services/fongo-home-phone/ <br>\n",
    "Bell Pricing: https://www.bell.ca/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db910f7b-fb55-4d2c-8020-f3f756c53779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Range check\n",
    "\n",
    "# Dictionary containing every feature's minimum and maximum valid values\n",
    "range_dict = {\n",
    "    \"tenure\": (0, 900),\n",
    "    \"MonthlyCharges\": (5, 297),\n",
    "    \"TotalCharges\": (0, 267300)\n",
    "}\n",
    "\n",
    "# Evaluates a single value against a given range\n",
    "def range_filter(value, minimum, maximum):\n",
    "    try:\n",
    "        value = float(value)\n",
    "    except Exception as e:\n",
    "        return False\n",
    "    return minimum <= value <= maximum\n",
    "\n",
    "# Apply the function to every feature in the dictionary, setting out of range values to True\n",
    "for feature in range_dict.keys():\n",
    "    invalid_range = cleaned_telco_df[feature].apply(\n",
    "        lambda attribute: not range_filter(attribute, range_dict[feature][0], range_dict[feature][1])\n",
    "    )\n",
    "    # Print the number of rows with a value that is not stored as the correct datatype for the designated attribute\n",
    "    print(f\"Number of rows where the {feature} value is not in the correct range {range_dict[feature]}: {invalid_range.sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80e31b7-b4c3-4619-afc3-ef1d6f7583f4",
   "metadata": {},
   "source": [
    "Since none of the values fail the range check, no cleaning is required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9303d6c-c099-431a-8ab0-d4034e6d5dc3",
   "metadata": {},
   "source": [
    "### (b) Numerical Feature Encoding\n",
    "\n",
    "Decision trees can benefit from binning numerical features. Our professor often references `Samy Baladram` articles in our lecture notes, so we will be following his guide to discretization, linked in our references. There are two methods he mentions in this article that we will use:\n",
    "\n",
    "**1. Equal-Frequency-Binning (Quantile Binning):** This approach creates bins that contain roughly the same number of observations, which is good when you want to make sure the representation of data is balanced. This ensures decision boundaries are meaningful. <br>\n",
    "**2. Custom Binning:** This approach allows you to create bins based on domain knowledge. In our case this is useful because we can group `tenure` by the number of years, a logical representation.\n",
    "\n",
    "We will bin the three numerical features as follows:\n",
    "- `tenure`: We will use **custom binning** for tenure, as a logical way to bin `tenure` is based on the number of years the customer has been with the company. We will divide `tenure` into 3 bins. Note that we chose these bin ranges, as they provide a roughly even distribution of the data, so we are still roughly adhering to **equal-frequency-binning** as well.\n",
    "    - `Short`: The customer has been with the company for under 12 months. (`2069` customers)\n",
    "    - `Medium`: The customer has been with the company between 12 and 48 months. (`2671` customers)\n",
    "    - `Long`: The customer has been with the company for more than 48 months. (`2303` customers)\n",
    "- `MonthlyCharges`: There is not a logical custom way to bin the monthly charges, so we will simply bin based on quantile, which uses the **Quantile Binning** approach. We will divide `MonthlyCharges` into 3 bins, based on their tertile.\n",
    "    - `Low`: The customer's `MonthlyCharges` value is in the first tertile.\n",
    "    - `Medium`: The customer's `MonthlyCharges` value is in the second tertile.\n",
    "    - `High`: The customer's `MonthlyCharges` value is in the third tertile.\n",
    "- `TotalCharges`: There is not a logical custom way to bin the total charges, so we will simply bin based on quantile, which uses the **Quantile Binning** approach. We will divide `TotalCharges` into 3 bins, based on their tertile.\n",
    "    - `Low`: The customer's `TotalCharges` value is in the first tertile.\n",
    "    - `Medium`: The customer's `TotalCharges` value is in the second tertile.\n",
    "    - `High`: The customer's `TotalCharges` value is in the third tertile.\n",
    "\n",
    "We make a copy of the DataFrame where we store the binned data. We do not replace the numerical data, as it will be used later for outlier removal and feature extraction. Thus, new features are created for the binned data, called `binned_tenure`, `binned_MonthlyCharges`, and `binned_TotalCharges` respectively. The original numerical data will be dropped after feature extraction, before the model training.\n",
    "\n",
    "**References:** <br>\n",
    "Binning Techniques: https://medium.com/towards-data-science/discretization-explained-a-visual-guide-with-code-examples-for-beginners-f056af9102fa <br>\n",
    "Using qcut for Quantile Binning: https://pbpython.com/pandas-qcut-cut.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e16e06-990b-48dc-9101-6a4cc7b9d142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binning\n",
    "\n",
    "# Create a copy of the DataFrame that will store binned data\n",
    "binned_telco_df = cleaned_telco_df.copy()\n",
    "\n",
    "# Bin the tenure of a customer\n",
    "def bin_tenure(tenure):\n",
    "    # If the customer has been with Telco for less than a year, then their tenure is relatively short in length\n",
    "    if tenure < 12:\n",
    "        return \"Short\"\n",
    "    # If the customer has been with Telco between one and four years, then their tenure is relatively medium in length\n",
    "    elif tenure < 48:\n",
    "        return \"Medium\"\n",
    "    # If the customer has been with Telco for four or more years, then their tenure is relatively long in length\n",
    "    return \"Long\"\n",
    "\n",
    "# Apply the bin_tenure function to bin the tenure feature\n",
    "binned_telco_df[\"binned_tenure\"] = binned_telco_df[\"tenure\"].apply(bin_tenure)\n",
    "\n",
    "# Check the distribution, since this is custom logic\n",
    "print(f\"{binned_telco_df['binned_tenure'].value_counts()} \\n\")\n",
    "\n",
    "# Create labels for the MonthlyCharges and TotalCharges bins\n",
    "bin_labels = [\"Low\", \"Medium\", \"High\"]\n",
    "\n",
    "# Bin the MonthlyCharges based on tertile\n",
    "binned_telco_df[\"binned_MonthlyCharges\"] = pd.qcut(binned_telco_df[\"MonthlyCharges\"], q=3, labels=bin_labels)\n",
    "\n",
    "# Verify the distribution\n",
    "print(f\"{binned_telco_df['binned_MonthlyCharges'].value_counts()} \\n\")\n",
    "\n",
    "# Bin the TotalCharges based on tertile\n",
    "binned_telco_df[\"binned_TotalCharges\"] = pd.qcut(binned_telco_df[\"TotalCharges\"], q=3, labels=bin_labels)\n",
    "\n",
    "# Verify the distribution\n",
    "print(f\"{binned_telco_df[\"binned_TotalCharges\"].value_counts()} \\n\")\n",
    "\n",
    "# Show the first three rows of the binned DataFrame\n",
    "print(\"The first three rows of the binned DataFrame:\")\n",
    "binned_telco_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa2bf25-f775-487f-a355-1496164a2ec7",
   "metadata": {},
   "source": [
    "Thus, we have successfully binned our numerical features into roughly even-sized categories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8441782c-8b94-412a-a05d-71e22423c4be",
   "metadata": {},
   "source": [
    "### (f) Empirical Study\n",
    "\n",
    "We are only doing part a of the empirical study here, as it must precede the other sections of the assignment, as per the professor's announcement.\n",
    "\n",
    "#### Test and Train Set Split\n",
    "\n",
    "Before moving on to parts c through e, we must do the first step of part f—splitting the data into a training set and a test set. This is important, as we do not want to remove outliers from or add features to the test set. We should only be modifying the training set. We will perform the split on the binned DataFrame.\n",
    "\n",
    "We split the data such that 80% is used for the training set, and the remaining 20% is reserved as the test set. As mentioned above, the modifications done to the data to train different versions of the model (outlier removal and feature extraction) will only be done on the training sets. After making the modifications to the training data, we will perform 4-fold cross-validation on the modified training sets. This involves creating 4 validation sets from the modified training sets. Ultimately, we do not separate a validation set in this step, as specified by the professor.\n",
    "\n",
    "**References:** <br>\n",
    "Train Test Split: https://www.statology.org/pandas-train-test/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3f117d-0e09-4a2b-a033-77a576d882ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the DataFrame into training set and test set\n",
    "train_telco_df, test_telco_df = train_test_split(binned_telco_df, test_size=0.2, random_state=0)\n",
    "\n",
    "# Show that the DataFrame split properly\n",
    "print(f\"The number of rows in the training set is {len(train_telco_df)}, which is {round(100*len(train_telco_df)/len(binned_telco_df))}% of the data.\")\n",
    "print(f\"The number of rows in the test set is {len(test_telco_df)}, which is {round(100*len(test_telco_df)/len(binned_telco_df))}% of the data.\\n\")\n",
    "\n",
    "# Show some example rows of the training set\n",
    "print(f\"Example rows of the training set: \")\n",
    "train_telco_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e33f2e3-7a0f-4614-8bbf-6d97202ca9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show some example rows of the test set\n",
    "print(f\"Example rows of the test set: \")\n",
    "test_telco_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7cd7108-5d9b-4631-9dc3-76e7ae493964",
   "metadata": {},
   "source": [
    "Now that the data is succesfully partitioned into a training set and a test set, we will set aside the test set. Moving forwards, our EDA, outlier detection and removal, and feature extraction will be exclusively performed on the training set, `train_telco_df`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027d3730-14ed-4c57-974c-54367dbf287f",
   "metadata": {},
   "source": [
    "### (c) EDA and Outlier Detection\n",
    "\n",
    "#### Program LOF for Outlier Detection\n",
    "\n",
    "We will be using the built-in `LocalOutlierFactor` function from `scikit-learn`, which the professor confirmed is acceptable for this assignment.\n",
    "\n",
    "\n",
    "#### Outlier Detection for Categorical Features\n",
    "\n",
    "Outlier detection is generally done on numerical features. Furthermore, LOF can only be used on numerical features. Thus, categorical variables do not necessarily need to be checked for outliers in this assignment. However, we will still precede our numerical outlier analysis with a brief consideration of categorical features. To do so, we will print the value counts of the categories for each categorical feature, since it is simple to implement, and it is useful verify there are no categories that are minimally represented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad039bc-27b8-49b6-91e6-445e1024bad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical Feature Outlier Detection\n",
    "\n",
    "# List of categorical features\n",
    "categorical_features = [\n",
    "    \"gender\", \"SeniorCitizen\", \"Partner\", \"Dependents\", \"PhoneService\", \"MultipleLines\", \n",
    "    \"InternetService\", \"OnlineSecurity\", \"OnlineBackup\", \"DeviceProtection\", \"TechSupport\", \n",
    "    \"StreamingTV\", \"StreamingMovies\", \"Contract\", \"PaperlessBilling\", \"PaymentMethod\", \"Churn\"\n",
    "]\n",
    "\n",
    "for feature in categorical_features:\n",
    "    print(f\"{train_telco_df[feature].value_counts()}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0467d2c-cc4a-4b68-97be-cdef46375114",
   "metadata": {},
   "source": [
    "Clearly, none of the categories have an unreasonably low count, so there are no outliers in the categorical features. Now we may proceed with the EDA and outlier detection of numerical features, the focal point of the outlier detection in this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7639c354-c015-4bb4-bf90-ed9082374792",
   "metadata": {},
   "source": [
    "#### Outlier Detection for Numerical Features\n",
    "\n",
    "The focus of outlier detection in this assignment is on detecting outliers in the numerical features. There are three numerical features in this dataset, including `tenure`, `MonthlyCharges`, and `TotalCharges`.\n",
    "\n",
    "##### Exploratory Data Analysis\n",
    "\n",
    "Since LOF is computationally costly, we will first use EDA to visualize the three numerical features to determine which feature to use LOF on. We will use histograms to and boxplots to visualize the distribution of the data. First, we create a histogram to visualize the distribution of `tenure` values. We use the Freedman-Diaconis rule to determine the optimal number of bins for all the histograms. Note that we must use the original numerical values to generate new bins for the histograms, as the previously binned data uses quantile binning—that would not be useful for analyzing the distribution of the data.\n",
    "\n",
    "**References:** <br>\n",
    "Freedman-Diaconis: https://stats.stackexchange.com/questions/798/calculating-optimal-number-of-bins-in-a-histogram <br>\n",
    "Gridlines: https://www.w3schools.com/python/matplotlib_grid.asp <br>\n",
    "Histplot: https://seaborn.pydata.org/generated/seaborn.histplot.html <br>\n",
    "Boxplot: https://seaborn.pydata.org/generated/seaborn.boxplot.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b084196-aa6e-439a-aa81-c0ff1cbadee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the tenure distribution\n",
    "\n",
    "# Use Freedman-Diaconis rule to determine number of bins\n",
    "x_axis = \"tenure\"\n",
    "iqr = np.percentile(train_telco_df[x_axis], 75) - np.percentile(train_telco_df[x_axis], 25)\n",
    "bin_width = 2 * iqr / (len(train_telco_df[x_axis]) ** (1/3))\n",
    "num_bins = int((train_telco_df[x_axis].max() - train_telco_df[x_axis].min()) / bin_width)\n",
    "\n",
    "# Create the histogram of tenure\n",
    "sns.histplot(data=train_telco_df, x=x_axis, bins=num_bins, color=\"darkseagreen\")\n",
    "\n",
    "# Label the histogram\n",
    "plt.xlabel(x_axis.title() + \" (Months)\")\n",
    "plt.ylabel('Number of Customers')\n",
    "plt.title('Histogram of the ' + x_axis.title())\n",
    "plt.grid()\n",
    "\n",
    "# Display plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8dcc7a2-d677-4837-a59c-eedd7a8355e2",
   "metadata": {},
   "source": [
    "There are clearly no outliers in the `tenure` data. The values are distributed relatively evenly; there are no extreme values that are isolated. The large counts of low and high `tenure` appear to be natural, as they represent new customers and very old customers respectively; if the company is about 72 months old, it would make sense that there is a large count of people who joined at the same time when the company was first founded. Another EDA method to determine if there are outliers is to use a boxplot. The boxplot will use the IQR to help visualize outliers. The whiskers occur at `Q1 - 1.5 * IQR`, and `Q3 + 1.5 * IQR`. Any values below the first whisker or above the second whisker are considered outliers, and are marked with dots on the plot. We will now create a boxplot for the `tenure` feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114d1cc1-bb64-42a3-b7f2-3729369d8b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a boxplot for Tenure\n",
    "sns.boxplot(x=train_telco_df[\"tenure\"])\n",
    "plt.title(\"Boxplot of Tenure\")\n",
    "plt.xlabel(\"Tenure (Months)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966d7e26-3db0-4d26-b20b-bfd3ee221237",
   "metadata": {},
   "source": [
    "All of the values are contained within the whiskers, so there are no outliers in the `tenure` feature. This supports our previous analysis of the histogram. We move on to visualizing `MonthlyCharges` with a histogram next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d9f835-99ed-450e-8541-df903cbad604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the MonthlyCharges distribution\n",
    "\n",
    "# Use Freedman-Diaconis rule to determine number of bins\n",
    "x_axis = \"MonthlyCharges\"\n",
    "iqr = np.percentile(train_telco_df[x_axis], 75) - np.percentile(train_telco_df[x_axis], 25)\n",
    "bin_width = 2 * iqr / (len(train_telco_df[x_axis]) ** (1/3))\n",
    "num_bins = int((train_telco_df[x_axis].max() - train_telco_df[x_axis].min()) / bin_width)\n",
    "\n",
    "# Create the histogram of MonthlyCharges\n",
    "sns.histplot(data=train_telco_df, x=x_axis, bins=num_bins, color=\"darkseagreen\")\n",
    "\n",
    "# Label the histogram\n",
    "plt.xlabel(\"Monthly Charges ($)\")\n",
    "plt.ylabel('Number of Customers')\n",
    "plt.title('Histogram of the Monthly Charges')\n",
    "plt.grid()\n",
    "\n",
    "# Display plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcff1d94-38a5-4606-85aa-cd18179d13aa",
   "metadata": {},
   "source": [
    "There are no outliers for the `MonthlyCharges` either. Similar to `tenure`, there are no extreme values that are clearly isolated. The distribution makes sense given the business logic as well. The spike around 20 dollars makes sense, as a significant number of customers will simply purchase the cheapest option. There are a lower amount of customers near 120, but a monthly payment near 120 is reasonable and expected—from our previous industry research, Bell has many internet plans that are more costly than 120 dollars. Given that there is still a fair number of customers near 120 dollars, a price near 120 dollars makes sense in the business context, and there are no gaps in the bars directly preceding the bar near 120 dollars (it is not isolated from the rest of the data), we conclude that there are no outliers in the `MonthlyCharges`. We will now create a boxplot for the `MonthlyCharges` feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ed2199-bf06-42ca-8166-95cf7e592de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a boxplot for MonthlyCharges\n",
    "sns.boxplot(x=train_telco_df[\"MonthlyCharges\"])\n",
    "plt.title(\"Boxplot of Monthly Charges\")\n",
    "plt.xlabel(\"Monthly Charges ($)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8223fe-7459-4f74-81a6-7f5b671f82c0",
   "metadata": {},
   "source": [
    "All of the values are contained within the whiskers, so there are no outliers in the `MonthlyCharges` feature. This supports our previous analysis of the histogram. We move on to visualizing `TotalCharges` next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2107b5a0-9182-4671-94ac-3c8bf3aaf419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the TotalCharges distribution\n",
    "\n",
    "# Use Freedman-Diaconis rule to determine number of bins\n",
    "x_axis = \"TotalCharges\"\n",
    "iqr = np.percentile(train_telco_df[x_axis], 75) - np.percentile(train_telco_df[x_axis], 25)\n",
    "bin_width = 2 * iqr / (len(train_telco_df[x_axis]) ** (1/3))\n",
    "num_bins = int((train_telco_df[x_axis].max() - train_telco_df[x_axis].min()) / bin_width)\n",
    "\n",
    "# Create the histogram of TotalCharges\n",
    "sns.histplot(data=train_telco_df, x=x_axis, bins=num_bins, color=\"darkseagreen\")\n",
    "\n",
    "# Label the histogram\n",
    "plt.xlabel(\"Total Charges ($)\")\n",
    "plt.ylabel('Number of Customers')\n",
    "plt.title('Histogram of the Total Charges')\n",
    "plt.grid()\n",
    "\n",
    "# Display plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcec3919-f3c4-472a-9fbf-b1bce0f4f49c",
   "metadata": {},
   "source": [
    "The histogram of `TotalCharges` shows a right-skewed distribution, but there do not seem to be outliers. There are no data points that differ significantly from their neighbours; there are no isolated bars, and it is a very steady decline. From a business context, it also makes sense that as more customers have lower `tenure` and lower `MonthlyCharges`, the `TotalCharges` counts would gradually decline as the `TotalCharges` increase. Furthermore, the actual value of a little over 8000 dollars would make sense for customers that have been paying a little over a hundred dollars a month for the full 72 months. Let us show some example rows to determine if these large values are natural."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab9e0b0-ef2d-4586-b5c2-a1cb66e558a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_telco_df[train_telco_df[\"TotalCharges\"] > 8000].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a065b890-a36a-49d5-b498-2f1869063d60",
   "metadata": {},
   "source": [
    "We are only including 3 rows in the final notebook for brevity, but that can easily be changed by changing the head value. When analyzing the first 20 rows, we note that all the customers have a `tenure` over 70 months and a `MonthlyCharge` over 100 dollars. This makes sense in the business context, so the larger values are natural.\n",
    "\n",
    "We want to explicitly recognize that the frequency counts of the higher `TotalCharges` are relatively low. We recognize that naturally occuring values can still be outliers. Thus, even though we asserted these values are naturally occuring, that does not confirm that they are not outliers. However, in addition to the fact that they are naturally occurring, since there is a continuous gradual decline in values, and there are no isolated values, we do not consider them outliers. Another great way to check if they are outliers is to visualize the data with a boxplot. We will now create a boxplot for the `TotalCharges` feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c1267f-ffd3-46b8-9bdf-b62c858df90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a boxplot for TotalCharges\n",
    "sns.boxplot(x=train_telco_df[\"TotalCharges\"])\n",
    "plt.title(\"Boxplot of Total Charges\")\n",
    "plt.xlabel(\"Total Charges ($)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d7a709-8563-4581-afe4-1eb07a43c0a2",
   "metadata": {},
   "source": [
    "All of the values are contained within the whiskers, so there are no outliers in the `TotalCharges` feature. This supports our previous analysis of the histogram."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2be9508-7e29-4c2c-996e-3c6e5998c562",
   "metadata": {},
   "source": [
    "##### Outlier Detection Using LOF\n",
    "\n",
    "In the assignment description, it states that if there are no features with outliers in our dataset, then we should purposefully introduce some so that LOF can detect them. It is important to note that, despite my high-level EDA analysis using histograms and the interquartile range check indicating that there are no outliers, that does not negate the possibility that the LOF algorithm could classify some points as outliers. In particular, since the `TotalCharges` feature is heavily right-skewed, it is possible that LOF would consider some of the higher `TotalCharges` values as outliers. Thus, we will first check if LOF detects outliers in the `TotalCharges` feature before inserting our own artificial outliers.\n",
    "\n",
    "**References:** <br>\n",
    "Numpy Array Value Counts: https://stackoverflow.com/questions/10741346/frequency-counts-for-unique-values-in-a-numpy-array <br>\n",
    "LOF: https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.LocalOutlierFactor.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d6fd4f-ebfa-4c43-93fb-df7c6cda2741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect outliers using LOF\n",
    "\n",
    "# Store the TotalCharges values in a 2D array\n",
    "X = train_telco_df[[\"TotalCharges\"]].values\n",
    "\n",
    "# Run LOF on the TotalCharges values \n",
    "clf = LocalOutlierFactor(n_neighbors=20)\n",
    "y_pred = clf.fit_predict(X)\n",
    "\n",
    "# Check if LOF found any outliers (outliers will be marked as -1, and inliers are marked as 1)\n",
    "unique, counts = np.unique(y_pred, return_counts=True)\n",
    "np.asarray((unique, counts)).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b5a582-7d59-47c0-89f3-4576d47fa3fd",
   "metadata": {},
   "source": [
    "Thus, despite my EDA analysis through histograms and boxplots (IQR analysis), LOF did in fact detect `68` outliers. We discussed this with the professor, and she stated that even if we believe these are natural and not outliers, if LOF detects them as outliers, we should still either remove or impute them. We can then see if their removal had a positive or negative impact on the model at evaluation time, which can be used to confirm or deny our intuition. Thus, we do not need to introduce synthetic outliers, as we can use the `68` outliers detected by LOF. Let us investigate which points were identified as outliers before we make a decision on either removal or imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b09ade3-2bef-4061-a346-b90449553f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the TotalCharges outliers distribution\n",
    "\n",
    "# Convert the array of -1s and 1s into a boolean mask to filter the rows\n",
    "total_charges_outlier_mask = y_pred == -1\n",
    "\n",
    "# Use the mask to save the outlier rows\n",
    "total_charges_outliers = train_telco_df[total_charges_outlier_mask]\n",
    "\n",
    "# Use Freedman-Diaconis rule to determine number of bins\n",
    "x_axis = \"TotalCharges\"\n",
    "iqr = np.percentile(total_charges_outliers[x_axis], 75) - np.percentile(total_charges_outliers[x_axis], 25)\n",
    "bin_width = 2 * iqr / (len(total_charges_outliers[x_axis]) ** (1/3))\n",
    "num_bins = int((total_charges_outliers[x_axis].max() - total_charges_outliers[x_axis].min()) / bin_width)\n",
    "\n",
    "# Create the histogram of TotalCharges\n",
    "sns.histplot(data=total_charges_outliers, x=x_axis, bins=num_bins, color=\"darkseagreen\")\n",
    "\n",
    "# Label the histogram\n",
    "plt.xlabel(\"Total Charges ($)\")\n",
    "plt.ylabel('Number of Customers')\n",
    "plt.title('Histogram of the Total Charges Outliers')\n",
    "plt.grid()\n",
    "\n",
    "# Display plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c842302-efe3-4634-8250-4fc523a13558",
   "metadata": {},
   "source": [
    "As expected, some of the outliers are in fact the large `TotalCharges` values. What is more surprising, however, is that a significant number of the outliers are the lowest `TotalCharges` values. Let us view some examples of exact data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8ae61e-5e80-4e5a-9049-ff2fd7e41bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show some example outlier values of TotalCharges\n",
    "total_charges_outliers[\"TotalCharges\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283b0476-4af3-40b8-b7a6-f8379d136f99",
   "metadata": {},
   "source": [
    "It seems as though the outliers are mostly below a value of 100 dollars. \n",
    "\n",
    "##### Deciding What to do With Outliers\n",
    "\n",
    "Ultimately, since the data points are at the extremities (and we know they are natural, real, values), it does not make sense to impute them with a new value. Thus, we will simply remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3920ed12-28f4-4acc-9bd5-03770527ef1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the outliers\n",
    "\n",
    "# Use the mask to save the inliers as a new DataFrame\n",
    "outliers_removed_train_telco_df = train_telco_df[~total_charges_outlier_mask]\n",
    "\n",
    "# Verify the length is the expected length of 5566\n",
    "len(outliers_removed_train_telco_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5f1ce5-2382-4820-a5d2-589e82299c73",
   "metadata": {},
   "source": [
    "Now, we have successfully removed the outliers from the DataFrame. Now there are two training sets that we will perform feature extraction on:\n",
    "- `train_telco_df`: The DataFrame with the LOF outliers left in.\n",
    "- `outliers_removed_train_telco_df`: The DataFrame without the LOF outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc8afc4-d28c-434f-85f2-fa4b16283285",
   "metadata": {},
   "source": [
    "### (d) Predictive Analysis: Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1aaff1-7da3-4c5c-a004-770e1bc6cca1",
   "metadata": {},
   "source": [
    "We explore the `DecisionTreeClassifier` method suggested in `scikit-learn`. As learned in class, the `DecisionTreeClassifier` creates a model that predicts the value of a target feature by learning decision rules from the input features. The function supports three different functions that measure the quality of the split: `gini`, which chooses the split that minimizes the gini impurity, and `entropy` and `log_loss`, which both choose the split that maximizes information gain.\n",
    "\n",
    "#### Usage\n",
    "There is an important prerequisite for using the `DecisionTreeClassifier` method; according to the `scikit-learn` docs, **the scikit-learn implementation does not support categorical variables for now.** Thus, we must encode our categorical variables using one-hot encoding to be able to train on them. Thus, we will add an encoding step in our preparation pipeline after binning, and before splitting the data into training and test sets. The original categorical columns will be immediately removed in the test set since they will no longer be used. However, the original categorical columns are used in our EDA, outlier detection, and feature extraction sections, so they will only be removed from the training sets immediately before running the actual `DecisionTreeClassifier` method in `part f` of the assignment.\n",
    "\n",
    "In terms of precise usage, you first initialize the `DecisionTreeClassifier` with `clf = DecisionTreeClassifier(random_state=0)`. Then, you can perform 4-fold cross-validation with `cross_val_score(clf, telco.data, telco.target, cv=4)`, where `telco.data` holds the input features, and `telco.target` holds the target feature. Scoring metrics can also be passed into `cross_val_score` for validation purposes.\n",
    "\n",
    "#### Parameters\n",
    "There are a number of parameters that can be set using the `DecisionTreeClassifier` method. We will provide an overview of the parameters as documented in the `scikit-learn` documentation, then we will specify our chosen baseline setting.\n",
    "- **criterion:** The function used to evaluate the splits.\n",
    "    - Default: `gini`\n",
    "    - Options: `gini`, `entropy`, `log_loss`\n",
    "- **splitter:** The strategy used to choose the split. The best split (maximizing information gain or minimizing impurity) is the default, but a random reasonable split can be selected as well.\n",
    "    - Default: `best`\n",
    "    - Options: `best`, `random`\n",
    "- **max_depth:** An integer specifying the maximum depth of the decision tree. It can also be `None`, which expands the tree until the leaf nodes are pure, or contain less samples than the specified `min_samples_split`.\n",
    "    - Default: `None`\n",
    "    - Options: int, `None`\n",
    "- **min_samples_split:** The minimum number of samples required in a node for a split.\n",
    "    - Default: 2\n",
    "    - Options: int or float\n",
    "- **min_samples_leaf**: The minimum number of samples required in a leaf. For example, if this is 3, all leaves must have at least 3 samples.\n",
    "    - Default: 1\n",
    "    - Options: int or float\n",
    "- **min_weight_fraction_leaf**: The minimum weighted fraction of the total weights required in a leaf.\n",
    "    - Default: 0.0\n",
    "    - Options: float\n",
    "- **max_features:** The number of features used when deciding on the split.\n",
    "    - Default: `None` (all features)\n",
    "    - Options: int, float, `sqrt`, `log2`, `None`\n",
    "- **random_state:** Controls the randomness. If set to an int, the decision tree training becomes deterministic.\n",
    "    - Default: `None` (random)\n",
    "    - Options: int, `RandomState` instance, `None`\n",
    "- **max_leaf_nodes:** Stop training once `max_leaf_nodes` leaf nodes have been created. Takes the `max_leaf_nodes` \"best\" leaf nodes.\n",
    "    - Default: `None`\n",
    "    - Options: int, `None`\n",
    "- **min_impurity_decrease:** A split will only occur if it decreases the impurity by at least `min_impurity_decrease`.\n",
    "    - Default: `0.0`\n",
    "    - Options: float\n",
    "- **class_weight:** Specifies how much weight to put on each target class during training.\n",
    "    - Default: `None` (equal weights)\n",
    "    - Options: dict, list of dict, `balanced`, `None`\n",
    "- **ccp_alpha:** Specifies how the tree should be pruned to avoid over-fitting.\n",
    "    - Default: 0.0\n",
    "    - Options: non-negative float\n",
    "- **monotonic_cst:** An array indicating monotonicity constraints on each feature (in other words, specifying if the target increases or decreases in relation to each feature).\n",
    "    - Default: `None` (no constraints)\n",
    "    - Options: Array of int of shape (n_features)\n",
    "\n",
    "For our baseline method, we will use the defaults for each. Our rationale is that even if specifying some of the parameters might improve the model, the baseline is not meant to be optimized for the dataset. It is meant to be a starting point that can be improved on. Thus, it makes sense to simply use the defaults for the baseline model. Optimizations can be made on the parameters through experimentation during our empirical study. We will briefly go over each default value and discuss why it should work for our baseline model, as well as our intuition on how it might influence the results.\n",
    "\n",
    "- **criterion:** `gini`\n",
    "    - `gini`, `entropy`, `log_loss`, are all applicable for classification using a decision tree. Thus, using the `gini` default will work perfectly fine on our dataset. According to the `Let's talk about data science blog`, `gini` does not handle unbalanced datasets as well as `entropy`. Since our target is relatively unbalanced (about one quarter `Yes`, three quarters `No`), this is a good reason to use `entropy` instead, but recall that this is our baseline method that is meant to represent an **unoptimized** model. Thus, we will still use the default of `gini`. We can evaluate our intuition that `entropy` is better during the parameter tuning stage of our empirical study.\n",
    "- **splitter:** `best`\n",
    "    - The `best` option for `splitter` is pretty standard, and splitting based on the `best` division should cause no problems in our dataset. In a worst case, it could overfit the tree to the training data. We can compare this option to `random` in the parameter tuning stage of our empirical study.\n",
    "- **max_depth:** `None`\n",
    "    - Not setting a `max_depth` is not unusual, imposing no issues. It could lead to very large trees, but our dataset is not particulary large so this should not be an issue. \n",
    "- **min_samples_split:** 2\n",
    "    - Splitting whenever there are still multiple values in a node will ensure as many splits are done as possible, which is acceptable. It could lead to overfitting; this can be evaluated when parameter tuning. It can also lead to very large trees, but our dataset is not particulary large so this should not be an issue. \n",
    "- **min_samples_leaf**: 1\n",
    "    - By not enforcing a minimum number of samples in a leaf, we are allowing the tree to grow as much as possible, which is reasonable. It could lead to overfitting; this can be evaluated when parameter tuning. It can also lead to very large trees, but our dataset is not particulary large so this should not be an issue. \n",
    "- **min_weight_fraction_leaf**: 0.0\n",
    "    - By not enforcing a minimum number of weight in a leaf, we are allowing the tree to grow as much as possible, which is reasonable. It could lead to overfitting; this can be evaluated when parameter tuning. It can also lead to very large trees, but our dataset is not particulary large so this should not be an issue. \n",
    "- **max_features:** `None` (all features)\n",
    "    - It is not unusual to use all the features when training a decision tree. This could lead to overfitting, though (especially since some of our features are somewhat redundant).\n",
    "- **random_state:** 0\n",
    "    - We are actually not using the default for `random_state`. Since we want our results to be reproducible, we set `random_state` to 0.\n",
    "- **max_leaf_nodes:** `None`\n",
    "    - By not enforcing a maximum number of leaf nodes, we are allowing the tree to grow as much as possible, which is reasonable. It could lead to overfitting; this can be evaluated when parameter tuning. It can also lead to very large trees, but our dataset is not particulary large so this should not be an issue.\n",
    "- **min_impurity_decrease:** `0.0`\n",
    "    - By not enforcing a minimum impurity on a split, we are allowing the tree to grow as much as possible, which is reasonable. It could lead to overfitting; this can be evaluated when parameter tuning. It can also lead to very large trees, but our dataset is not particulary large so this should not be an issue.\n",
    "- **class_weight:** `None` (equal weights)\n",
    "    - In theory, it would actually be better to used `balanced` since our dataset does not have an even split of the target classes. However, no weights on the class is standard, and we reiterate that we are meant to determine a starting point, not an optimal set of parameters. This can be adjusted in our empirical study.\n",
    "- **ccp_alpha:** 0.0\n",
    "    - By default, no pruning is performed, so we are allowing the tree to grow as much as possible, which is reasonable. It could lead to overfitting; this can be evaluated when parameter tuning. It can also lead to very large trees, but our dataset is not particulary large so this should not be an issue.\n",
    "- **monotonic_cst:** `None` (no constraints)\n",
    "    - We have no indication of any monotonic constraints that would benefit the model, so even if we were trying to optimize our baseline model, we would use `None` for this feature.\n",
    " \n",
    "So, the actual code used for our baseline method will look like `DecisionTreeClassifier(random_state=0)`, as the rest of the parameters are defaults.\n",
    "\n",
    "\n",
    "**References:** <br>\n",
    "Decision Tree Classifier: https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html <br>\n",
    "Decision Trees: https://scikit-learn.org/stable/modules/tree.html <br>\n",
    "Categorical Support: https://medium.com/@dyahayusekarkinasih/categorical-feature-in-decision-tree-classifier-3ad0c42c6dcc <br>\n",
    "Gini Versus Entropy: https://ekamperi.github.io/machine%20learning/2021/04/13/gini-index-vs-entropy-decision-trees.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b749f91-de9d-447d-b9c7-62a31897308b",
   "metadata": {},
   "source": [
    "### (e) Feature Engineering (TODO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ee202d-ad9b-4243-8d0d-750f66e6956f",
   "metadata": {},
   "source": [
    "average_monthly_charge = total charge / tenure\n",
    "- shows their average monthly charge\n",
    "\n",
    "monthly charge / average_monthly_charge\n",
    "- shows how their current rate compares to their historic rate\n",
    "- something below 1 means their charge went down, so they should be happy and less likely to leave\n",
    "- something above 1 means their charge went up, so they should be sad and more likely to leave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ea5370-43d6-4287-afe0-00f353a81b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the df that will contain the engineered features\n",
    "engineered_telco_df = cleaned_telco_df.copy()\n",
    "\n",
    "# Feature 1: Ratio comparing the user's current monthly charge to their historic average monthly charge\n",
    "\n",
    "def compute_current_to_historic_monthly_charge_ratio(row):\n",
    "    if row[\"tenure\"] == 0:\n",
    "        return 1\n",
    "    return row[\"MonthlyCharges\"]/(row[\"TotalCharges\"] / row[\"tenure\"])\n",
    "    \n",
    "\n",
    "engineered_telco_df[\"currentToHistoricMonthlyChargeRatio\"] = engineered_telco_df.apply(\n",
    "    lambda row: compute_current_to_historic_monthly_charge_ratio(row),\n",
    "    axis = 1\n",
    ")\n",
    "\n",
    "engineered_telco_df.head(3)\n",
    "\n",
    "engineered_telco_df[\"currentToHistoricMonthlyChargeRatio\"].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc37fec-1e4a-4f49-ac95-5ee67e95b48a",
   "metadata": {},
   "source": [
    "### (f) Empirical Study (TODO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aef5758-8ab4-480d-b044-50fd9e55567b",
   "metadata": {},
   "source": [
    "### (g) Results Analysis (TODO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55254c3f-74c5-41ad-bcc1-a71be7950f30",
   "metadata": {},
   "source": [
    "## Conclusion (TODO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a145387-7604-4b0e-b615-f1175bb3ecad",
   "metadata": {},
   "source": [
    "## References (TODO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3824b8e5-4fb1-432d-89d7-584690e9834a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
